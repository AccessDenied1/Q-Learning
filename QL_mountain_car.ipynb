{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ifv12bsPacFc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class QL:\n",
    "   def __init__(self,Q,policy,\n",
    "                legal_actions,\n",
    "                actions,\n",
    "                gamma,\n",
    "                lr):\n",
    "       self.Q = Q #Q matrix\n",
    "       self.policy =policy\n",
    "       self.legal_actions=legal_actions\n",
    "       self.actions = actions\n",
    "       self.gamma =gamma\n",
    "       self.lr =lr\n",
    "       \n",
    "   def q_value(self,s,a):\n",
    "       if (s,a) in self.Q:\n",
    "           self.Q[(s,a)]\n",
    "       else:\n",
    "           self.Q[s,a]=0\n",
    "       return self.Q[s,a]\n",
    "   def action(self,s):\n",
    "       if s in self.policy:\n",
    "           return self.policy[s]\n",
    "       else:\n",
    "           self.policy[s] = self.actions[np.random.randint(0,self.legal_actions)]\n",
    "       return self.policy[s]\n",
    "   def learn(self,s,a,s1,r,done):\n",
    "       if done== False:\n",
    "           self.Q[(s,a)] =self.q_value(s,a)+ self.lr*(r+self.gamma*max([self.q_value(s1,a1) for a1 in self.actions]) - self.q_value(s,a))\n",
    "       else:\n",
    "           self.Q[(s,a)] =self.q_value(s,a)+ self.lr*(r - self.q_value(s,a))\n",
    "       self.q_values = [self.q_value(s,a1) for a1 in self.actions]\n",
    "       self.policy[s] = self.actions[self.q_values.index(max(self.q_values))]\n",
    "\n",
    "\n",
    "Q = {}\n",
    "policy ={}\n",
    "legal_actions =3\n",
    "actions =[0,1,2]\n",
    "gamma = 0.99\n",
    "lr =0.5\n",
    "obj = QL(Q,policy,legal_actions,actions,gamma,lr)\n",
    "#obj.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "oOdBJgEXNhbs",
    "outputId": "80f0b5fb-5509-4c47-ec79-2975fd42a312"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "#Initialization of Environments\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "s = env.reset()\n",
    "\n",
    "#Hyper Parameters\n",
    "legal_actions=env.action_space.n\n",
    "actions = [0,1,2]\n",
    "gamma =0.99\n",
    "lr =0.7\n",
    "num_episodes =30000\n",
    "epsilon =0.6\n",
    "epsilon_decay =0.99\n",
    "###\n",
    "\n",
    "N_BINS = [10,10]\n",
    "\n",
    "MIN_VALUES = [0.6,0.07]\n",
    "MAX_VALUES = [-1.2,-.07]\n",
    "BINS = [np.linspace(MIN_VALUES[i], MAX_VALUES[i], N_BINS[i]) for i in range(len(N_BINS))]\n",
    "rList =[]\n",
    "def discretize(obs):\n",
    "       return tuple([int(np.digitize(obs[i], BINS[i])) for i in range(len(N_BINS))])\n",
    "\n",
    "#Q Matrix Parameters\n",
    "\n",
    "Q = {}\n",
    "policy ={}\n",
    "legal_actions =3\n",
    "actions =[0,1,2]\n",
    "gamma = 0.99\n",
    "lr =0.7\n",
    "QL = QL(Q,policy,legal_actions,actions,gamma,lr)\n",
    "\n",
    "#Training\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    s_raw= env.reset()\n",
    "    s = discretize(s_raw)\n",
    "    rAll =0\n",
    "    d = False\n",
    "    j = 0\n",
    "    for j in range(200):\n",
    "        \n",
    "        #epsilon greedy. to choose random actions initially when Q is all zeros\n",
    "        if np.random.random()< epsilon:\n",
    "            a = np.random.randint(0,legal_actions)\n",
    "            epsilon = epsilon*epsilon_decay\n",
    "        else:\n",
    "            a =QL.action(s)\n",
    "        s1_raw,r,d,_ = env.step(a)\n",
    "        rAll=rAll+r\n",
    "        s1 = discretize(s1_raw)\n",
    "        env.render()\n",
    "        if d:\n",
    "            if rAll<-199:\n",
    "                r =-100\n",
    "                QL.learn(s,a,s1,r,d)\n",
    "                print(\"Failed! Reward %d\"%rAll)\n",
    "            elif rAll>-199:\n",
    "                print(\"Passed! Reward %d\"%rAll)\n",
    "            break\n",
    "        QL.learn(s,a,s1,r,d)\n",
    "        if j==199:\n",
    "            print(\"Reward %d after full episode\"%(rAll))\n",
    "            \n",
    "        s = s1\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Q_learning _Copied.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
